{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc712c57",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13010725",
   "metadata": {},
   "source": [
    "##### Overfitting:\n",
    "##### A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise.\n",
    "##### Reasons for Overfitting:\n",
    "1. High variance and low bias.\n",
    "2. The model is too complex.\n",
    "3. The size of the training data.\n",
    "\n",
    "##### Techniques to Reduce Overfitting\n",
    "1. Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.\n",
    "2. Increase the training data can improve the model’s ability to generalize to unseen data and reduce the likelihood of overfitting.\n",
    "3. Reduce model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c91c9",
   "metadata": {},
   "source": [
    "##### Underfitting\n",
    "##### Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "##### Reasons for Underfitting\n",
    "1.\tThe model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "2.\tThe input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "3.\tThe size of the training dataset used is not enough.\n",
    "\n",
    "##### Techniques to Reduce Underfitting\n",
    "1.\tIncrease model complexity.\n",
    "2.\tIncrease the number of features, performing feature engineering.\n",
    "3.\tRemove noise from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1f4dd",
   "metadata": {},
   "source": [
    "### Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8bf547",
   "metadata": {},
   "source": [
    "##### To reduce overfitting:\n",
    "1. Hold-Out : Rather than using all of our data for training, we can simply split our dataset into two sets: training and testing. A common split ratio is 80% for training and 20% for testing. We train our model until it performs well not only on the training set but also for the testing set. This indicates good generalization capability since the testing set represents unseen data that were not used for training. \n",
    "2. Data augmentation: A larger dataset would reduce overfitting. If we cannot gather more data and are constrained to the data we have in our current dataset, we can apply data augmentation to artificially increase the size of our dataset. For example, if we are training for an image classification task, we can perform various image transformations to our image dataset (e.g., flipping, rotating, rescaling, shifting).\n",
    "3. Feature Selection: If we have only a limited amount of training samples, each with a large number of features, we should only select the most important features for training so that our model doesn’t need to learn for so many features and eventually overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5e14d",
   "metadata": {},
   "source": [
    "### Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c0977",
   "metadata": {},
   "source": [
    "Underfitting:\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "\n",
    "Underfitting occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. High bias and low variance are good indicators of underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e322a7",
   "metadata": {},
   "source": [
    "### Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907be70",
   "metadata": {},
   "source": [
    "Bias Variance Trade-Off:\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Bias Variance Trade-off. We try to optimize the value of the total error for the model by using the Bias-Variance Tradeoff.\n",
    "\n",
    "Total Error = Bias^2 + Variance + Irreducible Error\n",
    "\n",
    "The best fit will be given by the hypothesis on the tradeoff point.\n",
    "\n",
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2f21c",
   "metadata": {},
   "source": [
    "### Q5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817c9b1",
   "metadata": {},
   "source": [
    "1. Split the data: into three sets: training, validation, and test. The training set is used to fit the model, the validation set is used to tune the model parameters and select the best model, and the test set is used to evaluate the model performance on new data. By comparing the model performance on different sets, you can identify if the model is overfitting or underfitting. For example, if the model has a high accuracy on the training set but a low accuracy on the validation or test set, it is likely overfitting. If the model has a low accuracy on both the training and validation or test set, it is likely underfitting.\n",
    "2. Use cross-validation:By averaging the cross-validation scores, you can assess how well the model generalizes to new data. A low cross-validation score indicates underfitting, while a high variance between the cross-validation scores indicates overfitting.\n",
    "3. Plot learning curves: which are graphs that show how the model performance changes as a function of the training set size or the number of iterations. Learning curves can help you visualize how the model learns from the data and whether it suffers from high bias or high variance. A typical learning curve has two lines: one for the training score and one for the validation score. If the two lines are close and low, it means the model is underfitting. If the two lines are far apart and high, it means the model is overfitting. If the two lines converge and plateau, it means the model is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d241be",
   "metadata": {},
   "source": [
    "### Q6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d0eaa",
   "metadata": {},
   "source": [
    "A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data.\n",
    "Eg. \n",
    "1. Linear Regression: has high bias and low variance\n",
    "2. Decision Tree, Random Forest: have low bias and high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de980628",
   "metadata": {},
   "source": [
    "### Q7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33253cd8",
   "metadata": {},
   "source": [
    "Regularization is a method for increasing a model’s generalizability—that is, it’s ability to produce accurate predictions on new datasets.1 Regularization provides this increased generalizability at the sake of increased training error. It typically leads to less accurate predictions on training data but more accurate predictions on test data. This helps to reduce overfitting in machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff3bf0",
   "metadata": {},
   "source": [
    "Two forms of regularization are:\n",
    "1. L1 regularization: which adds the absolute values of the weights to the loss function. This encourages some weights to become exactly zero, effectively performing feature selection. It’s a handy tool when you suspect that only a subset of your features is essential.\n",
    "2. L2 regularization: which adds the square of the weights to the loss function. This tends to evenly distribute the importance across all features, reducing the magnitude of weights and preventing them from growing too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d8de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
